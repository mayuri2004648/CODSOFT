{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fb5164f-0fa8-4666-bd97-5669963ccb7d",
   "metadata": {},
   "source": [
    "Step 1: Install Necessary Libraries\n",
    "\n",
    "If you don't have the necessary libraries, install them using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa6d060-b50b-4f63-96ed-58ba369f670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\shree\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shree\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shree\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\shree\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shree\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b8edab-6d36-4b53-9b76-af5dd837f1a6",
   "metadata": {},
   "source": [
    "Step 2: Import Libraries\n",
    "\n",
    "Now, let's import the necessary libraries for data manipulation, visualization, and machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b668fcd2-7ba9-4afa-a0a5-0b4eca4b1a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03262a-68b9-49af-9c86-5271da3040a9",
   "metadata": {},
   "source": [
    "Step 3: Load the Titanic Dataset\n",
    "\n",
    "You can download the Titanic dataset from Kaggle's Titanic Competition. For this guide, let's assume the dataset is saved as titanic.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb6939f-253d-4c92-8965-85cad17d274d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "# Load the Titanic dataset\n",
    "df = pd.read_csv('Titanic-Dataset (1).csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a8dc2-9b1e-4307-9b62-e68feab170ef",
   "metadata": {},
   "source": [
    "Step 4: Data Exploration\n",
    "\n",
    "Before building the model, let's explore the dataset to understand its structure and check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0271386f-41db-43bc-b092-504c9e5fd557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "# Check basic info of the dataset\n",
    "print(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Display descriptive statistics of numeric columns\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9dc60-b6df-4863-ab07-7f66489b7833",
   "metadata": {},
   "source": [
    "Step 5: Data Preprocessing\n",
    "\n",
    "5.1: Handle Missing Data\n",
    "\n",
    "For columns like Age, Embarked, we need to handle missing values. We can fill missing Age with the median, and Embarked with the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4131b17-1633-4fb5-b7d3-4c2b675dc97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing 'Age' with the median\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "\n",
    "# Fill missing 'Embarked' with the most frequent value\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Check if there are any remaining missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bceded9-3575-4b40-84d5-efdc1664fae4",
   "metadata": {},
   "source": [
    "5.2: Feature Encoding\n",
    "\n",
    "For categorical variables like Sex, Embarked, we need to convert them into numeric format using Label Encoding or One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e9ea25-a34b-44bd-b769-59e3587f94e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name  Sex   Age  SibSp  Parch  \\\n",
      "0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n",
      "2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n",
      "4                           Allen, Mr. William Henry    0  35.0      0      0   \n",
      "\n",
      "             Ticket     Fare Cabin  Embarked  \n",
      "0         A/5 21171   7.2500   NaN         2  \n",
      "1          PC 17599  71.2833   C85         0  \n",
      "2  STON/O2. 3101282   7.9250   NaN         2  \n",
      "3            113803  53.1000  C123         2  \n",
      "4            373450   8.0500   NaN         2  \n"
     ]
    }
   ],
   "source": [
    "# Encode 'Sex' (Male = 0, Female = 1)\n",
    "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Encode 'Embarked' (C = 0, Q = 1, S = 2)\n",
    "df['Embarked'] = df['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "# Check the transformed dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c9739-c22a-4c1e-bc0a-ad5f48b63d28",
   "metadata": {},
   "source": [
    "5.3: Drop Irrelevant Features\n",
    "\n",
    "Some features like Name, Ticket, PassengerId may not provide useful information for the model. We can drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da2e486c-9862-47dc-a03c-50496583174c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass  Sex   Age  SibSp  Parch     Fare Cabin  Embarked\n",
      "0         0       3    0  22.0      1      0   7.2500   NaN         2\n",
      "1         1       1    1  38.0      1      0  71.2833   C85         0\n",
      "2         1       3    1  26.0      0      0   7.9250   NaN         2\n",
      "3         1       1    1  35.0      1      0  53.1000  C123         2\n",
      "4         0       3    0  35.0      0      0   8.0500   NaN         2\n"
     ]
    }
   ],
   "source": [
    "# Drop irrelevant columns\n",
    "df.drop(columns=['Name', 'Ticket', 'PassengerId'], inplace=True)\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cefa207-0a4b-4b20-8426-2405e5130a5a",
   "metadata": {},
   "source": [
    "Step 6: Split the Data\n",
    "\n",
    "Now that the data is clean, we can split it into features (X) and target (y), and further split it into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e10e69b6-9d8c-437d-9d63-20ca8fac7330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 8) (179, 8) (712,) (179,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the resulting datasets\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa119b0b-192f-4118-b3c4-83b9e3e95b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(f'Best Hyperparameters: {grid_search.best_params_}')\n",
    "\n",
    "# Re-train the model with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f'Best Model Accuracy: {accuracy_best * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374515a-f2a9-4ec5-8a27-978a619b1e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
