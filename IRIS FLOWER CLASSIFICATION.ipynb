{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f858f2e2-00a7-45c4-8bac-4d749e583a40",
   "metadata": {},
   "source": [
    "Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fb5f42-ae6f-4fea-bdac-284bfd8c8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.datasets import load_iris\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da0ee7-1e74-4cfd-a853-5d91512af711",
   "metadata": {},
   "source": [
    "Step 2: Load the Iris Dataset\n",
    "You can load the Iris dataset from sklearn.datasets or from a CSV file if you have it locally. Here, we'll load it using sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bded6cf2-5069-4c7b-a078-65dc4ff25024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "  species  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "\n",
    "# Map the target numbers to species names\n",
    "df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# Inspect the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d72db-10c2-4350-bf90-ab32af71dfae",
   "metadata": {},
   "source": [
    "Step 3: Data Exploration and Visualization\n",
    "Let's explore the dataset and visualize the distribution of the different Iris species.\n",
    "\n",
    "3.1 Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eea20c70-98ac-4ad3-9354-ccafd14f7c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
      "count         150.000000        150.000000         150.000000   \n",
      "mean            5.843333          3.057333           3.758000   \n",
      "std             0.828066          0.435866           1.765298   \n",
      "min             4.300000          2.000000           1.000000   \n",
      "25%             5.100000          2.800000           1.600000   \n",
      "50%             5.800000          3.000000           4.350000   \n",
      "75%             6.400000          3.300000           5.100000   \n",
      "max             7.900000          4.400000           6.900000   \n",
      "\n",
      "       petal width (cm)  \n",
      "count        150.000000  \n",
      "mean           1.199333  \n",
      "std            0.762238  \n",
      "min            0.100000  \n",
      "25%            0.300000  \n",
      "50%            1.300000  \n",
      "75%            1.800000  \n",
      "max            2.500000  \n",
      "species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get statistical summary of the data\n",
    "print(df.describe())\n",
    "\n",
    "# Check class distribution\n",
    "print(df['species'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03fdb3-bc02-48ca-9410-944cc7dbb316",
   "metadata": {},
   "source": [
    "3.2 Visualize Feature Distributions\n",
    "We'll use pair plots to visualize the relationships between the features and the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74820d09-0cf3-406f-a2fb-b1ad956ae743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shree\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "C:\\Users\\Shree\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "C:\\Users\\Shree\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "C:\\Users\\Shree\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    }
   ],
   "source": [
    "# Pairplot to visualize the feature distributions\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "sns.pairplot(df, hue='species', palette='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7afad9-6633-438a-a7a6-7b0d5576e628",
   "metadata": {},
   "source": [
    "Step 4: Data Preprocessing\n",
    "\n",
    "4.1 Split the Data into Features and Target\n",
    "The features are the first four columns, and the target is the last column (species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03b57c-8f64-41d6-8f42-76cb0a02b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (X) and the target variable (y)\n",
    "X = df.drop(columns=['species'])\n",
    "y = df['species']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6504d79-4027-4313-8810-c972585c0514",
   "metadata": {},
   "source": [
    "4.2 Train-Test Split\n",
    "\n",
    "We'll split the dataset into training and testing sets (80% train, 20% test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7dedee-13d8-4856-b4ac-a6a326123ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46e347-cbd5-4a55-bff7-322a17150b60",
   "metadata": {},
   "source": [
    "4.3 Feature Scaling (Optional)\n",
    "\n",
    "Feature scaling is important for certain machine learning algorithms like Logistic Regression and KNN. We will use StandardScaler to scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bde29d-6f92-496d-a1ca-4dc5056eab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (important for models like Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9b378d-d972-4967-a4fd-a4140a443a14",
   "metadata": {},
   "source": [
    "Step 5: Model Building\n",
    "\n",
    "We'll try two models: Logistic Regression and Random Forest Classifier.\n",
    "\n",
    "5.1 Logistic Regression\n",
    "python\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b28939-d6fd-43c7-9062-8b8aabb927b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "lr_model = LogisticRegression(max_iter=200)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74954ac5-1a85-4d3b-9329-83c6b0e24aab",
   "metadata": {},
   "source": [
    "5.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b5136-3b47-4946-b773-def4093e8aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963fb97d-6d93-4d14-af51-8682ad6a4fad",
   "metadata": {},
   "source": [
    "Step 6: Model Evaluation\n",
    "\n",
    "We will evaluate the models using Accuracy, Confusion Matrix, and Classification Report.\n",
    "\n",
    "6.1 Confusion Matrix and Classification Report for Logistic Regression\n",
    "python\n",
    "Copy code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c32f67-bedd-4829-80a4-c1576d70558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "sns.heatmap(conf_matrix_lr, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6693d9f-6f24-4b74-b3c7-69670057a71f",
   "metadata": {},
   "source": [
    "6.2 Confusion Matrix and Classification Report for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10adc40-0e53-4b11-882d-49494e74c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report (Random Forest):\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686dc72-987a-4c14-9088-245da6ac5f80",
   "metadata": {},
   "source": [
    "Step 7: Conclusion\n",
    "\n",
    "Based on the evaluation metrics, you can choose the model that performs best for your task. Typically, Random Forest should perform well due to its ability to handle non-linear relationships, but Logistic Regression is a simpler model that may also perform adequately.\n",
    "\n",
    "Final Model\n",
    "You can save the best-performing model using joblib for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b170fd-6836-42ae-8c84-cd34ff545109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the Random Forest model\n",
    "joblib.dump(rf_model, 'iris_flower_rf_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d14484-1989-41da-9f67-4b41c55e1aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
